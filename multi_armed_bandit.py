# -*- coding: utf-8 -*-
"""Multi-armed bandit

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17_Ul1ZgAkK3K7i8vMisrRJ8aYj4JmXZd
"""

import numpy as np
import scipy.stats as sp
import matplotlib.pyplot as plt

#Parameters
power = 0.8
confidence = 0.95
p = np.array([0.15, 0.25, 0.50])
num_bandits = len(p)
alternatives = list(range(num_bandits))
num_simulations = 5000

"""# AB Test"""

za = sp.norm.ppf((1 + confidence)/2)
zb = sp.norm.ppf(power)
sample_size = np.ceil((za + zb)**2*(p[0]*(1 - p[0]) + p[1]*(1 - p[1]))/(p[0] - p[1])**2)
sample_m1 = np.random.binomial(sample_size, p[0])/sample_size
sample_m2 = np.random.binomial(sample_size, p[1])/sample_size

rewards_ab = np.array([np.random.binomial(sample_size, prob, num_simulations) for prob in p])
rewards_ab = np.sum(rewards_ab, axis = 0)

"""# Random sampling"""

winner = np.random.choice(alternatives, num_simulations*num_bandits*int(sample_size))
winner = winner.reshape(num_simulations, num_bandits*int(sample_size))
reward = np.random.binomial(1, np.array(p)[winner])
cum_rewards_random = np.sum(reward, axis = 1)

"""# e-greedy"""

epsilon = 0.1
n = int(np.floor(sample_size*num_bandits/20))
cum_rewards = np.zeros(shape = (num_bandits, num_simulations))
runs = np.zeros(shape = (num_bandits, num_simulations))
prob_estimate = np.array([])

winner = np.random.choice(alternatives, n*num_simulations).reshape(num_simulations, n)
reward = np.random.binomial(1, np.array(p)[winner])

aux = np.ones(shape = winner.shape)
for bandit in range(num_bandits):
  location = winner == bandit
  cum_rewards[bandit] += np.sum(reward*location, axis = 1)
  runs[bandit] += np.sum(aux*location, axis = 1)
prob_estimate = cum_rewards/runs

for _ in range(n, int(num_bandits*sample_size)):
  #Exploit
  winner = np.argmax(prob_estimate, axis = 0)
  #Explore
  trial = np.random.rand(num_simulations) <= epsilon
  winner[trial] = np.random.choice(alternatives, np.sum(trial))
  reward = np.random.binomial(1, np.array(p)[winner])
  aux = np.ones(shape = winner.shape)
  for bandit in range(num_bandits):
    location = winner == bandit
    cum_rewards[bandit, location] += reward[location]
    runs[bandit, location] += 1
  prob_estimate = cum_rewards/runs

epsilon = 0.1
cum_rewards = np.array([np.random.binomial(2, prob, num_simulations) for prob in p])
runs = 2*np.ones(shape = (num_bandits, num_simulations))
prob_estimate = cum_rewards/runs

for _ in range(int(num_bandits*sample_size)):
  #Exploit
  winner = np.argmax(prob_estimate, axis = 0)
  #Explore
  trial = np.random.rand(num_simulations) <= epsilon
  winner[trial] = np.random.choice(alternatives, np.sum(trial))
  reward = np.random.binomial(1, np.array(p)[winner])
  aux = np.ones(shape = winner.shape)
  for bandit in range(num_bandits):
    location = winner == bandit
    cum_rewards[bandit, location] += reward[location]
    runs[bandit, location] += 1
  prob_estimate = cum_rewards/runs

rewards_greedy = np.sum(cum_rewards, axis = 0)

"""# Upper Confidence Bound"""

prob_estimate = np.array([])
cum_rewards = np.array([np.random.binomial(1, prob, num_simulations) for prob in p])
runs = np.ones(shape = (num_bandits, num_simulations))
prob_estimate = cum_rewards/runs
for t in range(num_bandits, int(num_bandits*sample_size)):
  UCB = prob_estimate + np.sqrt(2*np.log(t)/runs)
  winner = np.argmax(UCB, axis = 0)
  reward = np.random.binomial(1, p[winner])
  for bandit in range(num_bandits):
    location = winner == bandit
    cum_rewards[bandit, location] += reward[location]
    runs[bandit, location] += 1
  prob_estimate = cum_rewards/runs
rewards_ucb = np.sum(cum_rewards, axis = 0)

"""# Thompson Sampling"""

cum_rewards = np.zeros(num_simulations)
param_beta = np.array([[np.ones(num_simulations), np.ones(num_simulations)] for _ in range(num_bandits)])
for _ in range(num_bandits*int(sample_size)):
  teta = np.array([np.random.beta(param[0], param[1]) for param in param_beta])
  winner = np.argmax(teta, axis = 0)
  reward = np.random.binomial(1, p[winner])
  cum_rewards += reward
  for bandit in range(num_bandits):
    location = winner == bandit
    param_beta[bandit, 0, location] += reward[location]
    param_beta[bandit, 1, location] += 1 - reward[location]
rewards_thompson = cum_rewards[:]

"""# Plotting"""

plt.hist(cum_rewards_random, bins = 30, density = True, alpha = 0.6, label = 'random')
plt.hist(rewards_ab, bins = 30, density = True, alpha = 0.6, label = 'A/B Test')
plt.hist(rewards_thompson, bins = 30, density = True, alpha = 0.6, label = 'Thompson')
plt.hist(rewards_greedy, bins = 30, density = True, alpha = 0.6, label = 'Greedy')
plt.hist(rewards_ucb, bins = 30, density = True, alpha = 0.6, label = 'UCB')
plt.legend()
plt.vlines(sample_size*num_bandits*max(p), 0, 0.05)
plt.show()